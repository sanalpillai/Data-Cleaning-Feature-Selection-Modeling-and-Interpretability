{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh1A3eD3IRviFv22DSDR37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanalpillai/Data-Cleaning-Feature-Selection-Modeling-and-Interpretability/blob/main/Data_Cleaning_Feature_Selection_Modeling_and_Interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkNpDlNn8zjL"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import shap\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Loading and Initial Cleaning\n",
        "data_url = \"https://raw.githubusercontent.com/sanalpillai/Data-Cleaning-Feature-Selection-Modeling-and-Interpretability/main/Dataset/cleaned_data_cirrhosis.csv\"\n",
        "data = pd.read_csv(data_url)\n",
        "data['Status'] = data['Status'].replace({'CL': 'C'})\n",
        "data['Edema'] = data['Edema'].replace({'S': 'Y'})"
      ],
      "metadata": {
        "id": "XJhiNzNj-ZOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Handling Missing Values and Encoding\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "    else:\n",
        "        data[column].fillna(data[column].mean(), inplace=True)  # Replace with median if skewed\n",
        "\n",
        "data = pd.get_dummies(data)  # One-Hot Encoding for categorical variables"
      ],
      "metadata": {
        "id": "W2Q8qoLR-b1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Feature Normalization\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = data.select_dtypes(include=['float64', 'int']).columns\n",
        "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])"
      ],
      "metadata": {
        "id": "djHYnB0v-kD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Initializing H2O and Running AutoML\n",
        "h2o.init(max_mem_size=\"4G\")  # Adjust memory size according to your system's capacity\n",
        "h2o_df = h2o.H2OFrame(data)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = h2o_df.split_frame(ratios=[.75], seed=42)\n",
        "\n",
        "# Specify target and features\n",
        "target = \"Status\"  # Change according to your dataset\n",
        "features = [x for x in train.columns if x != target]\n",
        "\n",
        "# AutoML Configuration and Execution\n",
        "aml = H2OAutoML(max_models=20, seed=1, max_runtime_secs=600)\n",
        "aml.train(x=features, y=target, training_frame=train)"
      ],
      "metadata": {
        "id": "qxW3w5_Z-ksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Model Interpretability with SHAP\n",
        "best_model = h2o.get_model(aml.leaderboard[0, \"model_id\"])\n",
        "test_h2o = h2o.H2OFrame(X_test)  # Assuming X_test is your test set\n",
        "shap_values = best_model.predict_contributions(test_h2o)\n",
        "\n",
        "# Convert SHAP values to a pandas DataFrame for easier manipulation\n",
        "shap_values_df = shap_values.as_data_frame()\n",
        "shap.summary_plot(shap_values_df.to_numpy(), features=X_test, feature_names=X_test.columns)"
      ],
      "metadata": {
        "id": "_j1HKkCm-n9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}